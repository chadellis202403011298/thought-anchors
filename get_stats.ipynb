{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths\n",
    "analysis_dir = Path(\"analysis\")\n",
    "\n",
    "def load_analyzed_chunks(analysis_dir: Path) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Load all analyzed chunks from the analysis directory.\n",
    "    \n",
    "    Args:\n",
    "        analysis_dir: Path to the analysis directory\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries containing chunk data\n",
    "    \"\"\"\n",
    "    all_chunks = []\n",
    "    \n",
    "    # Iterate through all problem directories\n",
    "    for problem_dir in tqdm([d for d in analysis_dir.iterdir() if d.is_dir()]):\n",
    "        problem_id = problem_dir.name\n",
    "        \n",
    "        # Iterate through seed directories\n",
    "        for seed_dir in [d for d in problem_dir.iterdir() if d.is_dir()]:\n",
    "            seed = seed_dir.name\n",
    "            \n",
    "            # Load chunks_analyzed.json if it exists\n",
    "            chunks_file = seed_dir / \"chunks.json\"\n",
    "            if chunks_file.exists():\n",
    "                with open(chunks_file, 'r', encoding='utf-8') as f:\n",
    "                    chunks_data = json.load(f)\n",
    "                \n",
    "                # Add problem_id and seed to each chunk\n",
    "                for chunk in chunks_data:\n",
    "                    chunk['problem_id'] = problem_id\n",
    "                    chunk['seed'] = seed\n",
    "                    chunk['relative_position'] = chunk['index'] / len(chunks_data)\n",
    "                \n",
    "                all_chunks.extend([chunk for chunk in chunks_data if chunk['category'] != 'Unknown'])\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "# Load all analyzed chunks\n",
    "all_chunks = load_analyzed_chunks(analysis_dir)\n",
    "print(f\"Loaded {len(all_chunks)} chunks from {len(set(chunk['problem_id'] for chunk in all_chunks))} problems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_category_distribution(chunks: List[Dict]):\n",
    "    \"\"\"\n",
    "    Plot the distribution of reasoning categories.\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of chunk dictionaries\n",
    "    \"\"\"\n",
    "    # Count categories\n",
    "    categories = [chunk['category'] for chunk in chunks]\n",
    "    category_counts = Counter(categories)\n",
    "    \n",
    "    # Sort by frequency\n",
    "    sorted_categories = sorted(category_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Create DataFrame for plotting\n",
    "    df = pd.DataFrame(sorted_categories, columns=['Category', 'Count'])\n",
    "    \n",
    "    # Calculate percentage\n",
    "    total = sum(df['Count'])\n",
    "    df['Percentage'] = df['Count'] / total * 100\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    ax = sns.barplot(x='Category', y='Count', data=df)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for i, row in enumerate(df.itertuples()):\n",
    "        ax.text(i, row.Count + 5, f\"{row.Percentage:.1f}%\", ha='center')\n",
    "    \n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.title('Distribution of Reasoning Categories')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df\n",
    "\n",
    "category_distribution = plot_category_distribution(all_chunks)\n",
    "print(category_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_transition_matrix(chunks: List[Dict]):\n",
    "    \"\"\"\n",
    "    Compute transition matrix between reasoning categories.\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of chunk dictionaries\n",
    "        \n",
    "    Returns:\n",
    "        Transition matrix as DataFrame\n",
    "    \"\"\"\n",
    "    # Get unique categories\n",
    "    categories = sorted(set(chunk['category'] for chunk in chunks))\n",
    "    \n",
    "    # Initialize transition counts\n",
    "    transitions = defaultdict(Counter)\n",
    "    \n",
    "    # Group chunks by problem and seed\n",
    "    problem_chunks = defaultdict(list)\n",
    "    for chunk in chunks:\n",
    "        key = (chunk['problem_id'], chunk['seed'])\n",
    "        problem_chunks[key].append(chunk)\n",
    "    \n",
    "    # Count transitions within each problem\n",
    "    for key, chunks in problem_chunks.items():\n",
    "        # Sort chunks by index\n",
    "        sorted_chunks = sorted(chunks, key=lambda x: x['index'])\n",
    "        \n",
    "        # Count transitions\n",
    "        for i in range(len(sorted_chunks) - 1):\n",
    "            from_cat = sorted_chunks[i]['category']\n",
    "            to_cat = sorted_chunks[i + 1]['category']\n",
    "            transitions[from_cat][to_cat] += 1\n",
    "    \n",
    "    # Create transition matrix\n",
    "    matrix = []\n",
    "    for from_cat in categories:\n",
    "        row = []\n",
    "        total = sum(transitions[from_cat].values())\n",
    "        for to_cat in categories:\n",
    "            # Calculate probability if total > 0\n",
    "            prob = transitions[from_cat][to_cat] / total if total > 0 else 0\n",
    "            row.append(prob)\n",
    "        matrix.append(row)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(matrix, index=categories, columns=categories)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def plot_transition_matrix(transition_matrix: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Plot transition matrix as heatmap.\n",
    "    \n",
    "    Args:\n",
    "        transition_matrix: Transition matrix as DataFrame\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    sns.heatmap(\n",
    "        transition_matrix, \n",
    "        annot=True, \n",
    "        cmap='viridis', \n",
    "        vmin=0, \n",
    "        vmax=0.5,  # Cap at 0.5 for better color distribution\n",
    "        fmt='.2f'\n",
    "    )\n",
    "    plt.title('Transition Probabilities Between Reasoning Categories')\n",
    "    plt.xlabel('To Category')\n",
    "    plt.ylabel('From Category')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Compute and plot transition matrix\n",
    "transition_matrix = compute_transition_matrix(all_chunks)\n",
    "plot_transition_matrix(transition_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_category_positions(chunks: List[Dict]):\n",
    "    \"\"\"\n",
    "    Analyze the relative positions of each category in the reasoning chain.\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of chunk dictionaries\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with position statistics for each category\n",
    "    \"\"\"\n",
    "    # Group by category\n",
    "    category_positions = defaultdict(list)\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        category = chunk['category']\n",
    "        position = chunk['relative_position']\n",
    "        category_positions[category].append(position)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    stats = []\n",
    "    for category, positions in category_positions.items():\n",
    "        stats.append({\n",
    "            'Category': category,\n",
    "            'Count': len(positions),\n",
    "            'Mean Position': np.mean(positions),\n",
    "            'Std Dev': np.std(positions),\n",
    "            'Min': np.min(positions),\n",
    "            'Max': np.max(positions),\n",
    "            '25%': np.percentile(positions, 25),\n",
    "            '50%': np.percentile(positions, 50),\n",
    "            '75%': np.percentile(positions, 75)\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(stats)\n",
    "    df = df.sort_values('Mean Position')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def plot_category_positions(chunks: List[Dict]):\n",
    "    \"\"\"\n",
    "    Plot the distribution of category positions in the reasoning chain.\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of chunk dictionaries\n",
    "    \"\"\"\n",
    "    # Group by category\n",
    "    category_positions = defaultdict(list)\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        category = chunk['category']\n",
    "        position = chunk['relative_position']\n",
    "        category_positions[category].append(position)\n",
    "    \n",
    "    # Create DataFrame for plotting\n",
    "    data = []\n",
    "    for category, positions in category_positions.items():\n",
    "        for pos in positions:\n",
    "            data.append({'Category': category, 'Relative Position': pos})\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Sort categories by mean position\n",
    "    category_order = df.groupby('Category')['Relative Position'].mean().sort_values().index\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Box plot\n",
    "    ax = sns.boxplot(\n",
    "        x='Category', \n",
    "        y='Relative Position', \n",
    "        data=df, \n",
    "        order=category_order\n",
    "    )\n",
    "    \n",
    "    # Add scatter points for individual data points\n",
    "    sns.stripplot(\n",
    "        x='Category', \n",
    "        y='Relative Position', \n",
    "        data=df, \n",
    "        order=category_order,\n",
    "        size=4, \n",
    "        color='black', \n",
    "        alpha=0.3\n",
    "    )\n",
    "    \n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.title('Distribution of Category Positions in Reasoning Chain')\n",
    "    plt.ylabel('Relative Position (0=start, 1=end)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Return statistics\n",
    "    return analyze_category_positions(chunks)\n",
    "\n",
    "# Analyze and plot category positions\n",
    "position_stats = plot_category_positions(all_chunks)\n",
    "print(position_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_backtracking(chunks: List[Dict]):\n",
    "    \"\"\"\n",
    "    Analyze when backtracking occurs in the reasoning chain.\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of chunk dictionaries\n",
    "    \"\"\"\n",
    "    # Filter for backtracking chunks\n",
    "    backtracking_chunks = [chunk for chunk in chunks if chunk['category'] == 'Backtracking']\n",
    "    \n",
    "    if not backtracking_chunks:\n",
    "        print(\"No backtracking chunks found in the dataset.\")\n",
    "        return\n",
    "    \n",
    "    # Get positions\n",
    "    positions = [chunk['relative_position'] for chunk in backtracking_chunks]\n",
    "    \n",
    "    # Plot histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(positions, bins=20, alpha=0.7, color='blue')\n",
    "    plt.axvline(np.mean(positions), color='red', linestyle='dashed', linewidth=2, label=f'Mean: {np.mean(positions):.2f}')\n",
    "    \n",
    "    plt.title('Distribution of Backtracking in Reasoning Chain')\n",
    "    plt.xlabel('Relative Position (0=start, 1=end)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"Backtracking Statistics:\")\n",
    "    print(f\"  Count: {len(backtracking_chunks)}\")\n",
    "    print(f\"  Mean Position: {np.mean(positions):.3f}\")\n",
    "    print(f\"  Std Dev: {np.std(positions):.3f}\")\n",
    "    print(f\"  Min: {np.min(positions):.3f}\")\n",
    "    print(f\"  Max: {np.max(positions):.3f}\")\n",
    "    \n",
    "    # What categories typically precede backtracking?\n",
    "    preceding_categories = []\n",
    "    \n",
    "    # Group chunks by problem and seed\n",
    "    problem_chunks = defaultdict(list)\n",
    "    for chunk in chunks:\n",
    "        key = (chunk['problem_id'], chunk['seed'])\n",
    "        problem_chunks[key].append(chunk)\n",
    "    \n",
    "    # Find categories that precede backtracking\n",
    "    for key, prob_chunks in problem_chunks.items():\n",
    "        # Sort chunks by index\n",
    "        sorted_chunks = sorted(prob_chunks, key=lambda x: x['index'])\n",
    "        \n",
    "        # Find backtracking chunks\n",
    "        for i, chunk in enumerate(sorted_chunks):\n",
    "            if chunk['category'] == 'Backtracking' and i > 0:\n",
    "                preceding_categories.append(sorted_chunks[i-1]['category'])\n",
    "    \n",
    "    # Count preceding categories\n",
    "    preceding_counts = Counter(preceding_categories)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    categories, counts = zip(*preceding_counts.most_common())\n",
    "    \n",
    "    # Calculate percentages\n",
    "    total = sum(counts)\n",
    "    percentages = [count/total*100 for count in counts]\n",
    "    \n",
    "    # Create bars\n",
    "    bars = plt.bar(categories, percentages)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for bar, percentage in zip(bars, percentages):\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width()/2,\n",
    "            bar.get_height() + 1,\n",
    "            f\"{percentage:.1f}%\",\n",
    "            ha='center'\n",
    "        )\n",
    "    \n",
    "    plt.title('Categories That Precede Backtracking')\n",
    "    plt.xlabel('Category')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze backtracking\n",
    "analyze_backtracking(all_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_category_sequences(chunks: List[Dict], max_problems: int = 10):\n",
    "    \"\"\"\n",
    "    Visualize the sequence of categories for each problem.\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of chunk dictionaries\n",
    "        max_problems: Maximum number of problems to visualize\n",
    "    \"\"\"\n",
    "    # Get unique categories and assign colors\n",
    "    categories = sorted(set(chunk['category'] for chunk in chunks))\n",
    "    color_map = dict(zip(categories, sns.color_palette(\"husl\", len(categories))))\n",
    "    \n",
    "    # Group chunks by problem and seed\n",
    "    problem_chunks = defaultdict(list)\n",
    "    for chunk in chunks:\n",
    "        key = (chunk['problem_id'], chunk['seed'])\n",
    "        problem_chunks[key].append(chunk)\n",
    "    \n",
    "    # Select a subset of problems\n",
    "    selected_problems = random.sample(list(problem_chunks.keys()), max_problems)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(len(selected_problems), 1, figsize=(15, len(selected_problems) * 1.5))\n",
    "    if len(selected_problems) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Plot each problem\n",
    "    for i, (key, ax) in enumerate(zip(selected_problems, axes)):\n",
    "        problem_id, seed = key\n",
    "        prob_chunks = sorted(problem_chunks[key], key=lambda x: x['index'])\n",
    "        \n",
    "        # Create colored blocks for each category\n",
    "        for j, chunk in enumerate(prob_chunks):\n",
    "            category = chunk['category']\n",
    "            ax.barh(0, 1, left=j, color=color_map[category], alpha=0.7)\n",
    "            \n",
    "        # Set labels\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xlabel('Chunk Index')\n",
    "        ax.set_title(f'Problem: {problem_id}, Seed: {seed}')\n",
    "        ax.set_xlim(0, len(prob_chunks))\n",
    "    \n",
    "    # Create legend\n",
    "    handles = [plt.Rectangle((0,0),1,1, color=color_map[cat]) for cat in categories]\n",
    "    fig.legend(handles, categories, loc='upper center', bbox_to_anchor=(0.5, 0), \n",
    "               ncol=min(5, len(categories)), frameon=True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.15 + 0.02 * min(5, len(categories)))\n",
    "    plt.show()\n",
    "\n",
    "# Visualize category sequences\n",
    "visualize_category_sequences(all_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cosine_vs_correlation(analysis_dir: Path):\n",
    "    \"\"\"\n",
    "    Analyze the relationship between cosine similarity and Pearson correlation.\n",
    "    \n",
    "    Args:\n",
    "        analysis_dir: Path to the analysis directory\n",
    "    \"\"\"\n",
    "    # Store data for all problems\n",
    "    all_data = []\n",
    "    \n",
    "    # Iterate through all problem directories\n",
    "    for problem_dir in tqdm([d for d in analysis_dir.iterdir() if d.is_dir()]):\n",
    "        problem_id = problem_dir.name\n",
    "        \n",
    "        # Iterate through seed directories\n",
    "        for seed_dir in [d for d in problem_dir.iterdir() if d.is_dir()]:\n",
    "            seed = seed_dir.name\n",
    "            \n",
    "            # Check if both matrices exist\n",
    "            cosine_path = seed_dir / \"chunk_cosine_similarity.npy\"\n",
    "            corr_path = seed_dir / \"chunk_correlation.npy\"\n",
    "            chunks_path = seed_dir / \"chunks_analyzed.json\"\n",
    "            \n",
    "            if cosine_path.exists() and corr_path.exists() and chunks_path.exists():\n",
    "                # Load matrices\n",
    "                cosine_matrix = np.load(cosine_path)\n",
    "                corr_matrix = np.load(corr_path)\n",
    "                \n",
    "                # Load chunk data\n",
    "                with open(chunks_path, 'r', encoding='utf-8') as f:\n",
    "                    chunks_data = json.load(f)\n",
    "                \n",
    "                # Extract chunk categories\n",
    "                categories = [chunk['category'] for chunk in chunks_data]\n",
    "                abbreviations = [chunk['abbreviation'] for chunk in chunks_data]\n",
    "                \n",
    "                # Ensure matrices have the same shape\n",
    "                if cosine_matrix.shape == corr_matrix.shape:\n",
    "                    n = cosine_matrix.shape[0]\n",
    "                    \n",
    "                    # Extract all pairs (excluding self-comparisons)\n",
    "                    for i in range(n):\n",
    "                        for j in range(i+1, n):  # Only upper triangle\n",
    "                            all_data.append({\n",
    "                                'problem_id': problem_id,\n",
    "                                'seed': seed,\n",
    "                                'chunk_i': i,\n",
    "                                'chunk_j': j,\n",
    "                                'category_i': categories[i],\n",
    "                                'category_j': categories[j],\n",
    "                                'abbrev_i': abbreviations[i],\n",
    "                                'abbrev_j': abbreviations[j],\n",
    "                                'cosine_sim': cosine_matrix[i, j],\n",
    "                                'correlation': corr_matrix[i, j]\n",
    "                            })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(all_data)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load and analyze cosine vs correlation data\n",
    "cosine_corr_df = analyze_cosine_vs_correlation(analysis_dir)\n",
    "\n",
    "# Plot the relationship\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(\n",
    "    cosine_corr_df['cosine_sim'], \n",
    "    cosine_corr_df['correlation'],\n",
    "    alpha=0.5,\n",
    "    s=10\n",
    ")\n",
    "\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('Pearson Correlation')\n",
    "plt.title('Cosine Similarity vs Pearson Correlation for All Chunk Pairs')\n",
    "\n",
    "# Add reference lines\n",
    "plt.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "plt.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Add a diagonal line for reference\n",
    "x = np.linspace(*plt.xlim())\n",
    "plt.plot(x, x, color='red', linestyle='--', alpha=0.5, label='y=x')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate correlation between cosine similarity and Pearson correlation\n",
    "corr = cosine_corr_df['cosine_sim'].corr(cosine_corr_df['correlation'])\n",
    "print(f\"Correlation between cosine similarity and Pearson correlation: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_interesting_chunk_pairs(df: pd.DataFrame, n_pairs: int = 10):\n",
    "    \"\"\"\n",
    "    Find interesting chunk pairs with:\n",
    "    1. High cosine similarity but low correlation\n",
    "    2. Low cosine similarity but high correlation\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with cosine similarity and correlation data\n",
    "        n_pairs: Number of pairs to find in each category\n",
    "        \n",
    "    Returns:\n",
    "        DataFrames with interesting pairs\n",
    "    \"\"\"\n",
    "    # Calculate the difference between cosine similarity and correlation\n",
    "    df['cos_minus_corr'] = df['cosine_sim'] - df['correlation']\n",
    "    df['corr_minus_cos'] = df['correlation'] - df['cosine_sim']\n",
    "    \n",
    "    # Find pairs with high cosine but low correlation\n",
    "    high_cos_low_corr = df[\n",
    "        (df['cosine_sim'] > 0.7) &  # High cosine threshold\n",
    "        (df['correlation'] < 0.3)    # Low correlation threshold\n",
    "    ].sort_values('cos_minus_corr', ascending=False).head(n_pairs)\n",
    "    \n",
    "    # Find pairs with low cosine but high correlation\n",
    "    low_cos_high_corr = df[\n",
    "        (df['cosine_sim'] < 0.3) &   # Low cosine threshold\n",
    "        (df['correlation'] > 0.7)     # High correlation threshold\n",
    "    ].sort_values('corr_minus_cos', ascending=False).head(n_pairs)\n",
    "    \n",
    "    return high_cos_low_corr, low_cos_high_corr\n",
    "\n",
    "def plot_interesting_pairs(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Create a scatter plot highlighting interesting pairs.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with cosine similarity and correlation data\n",
    "    \"\"\"\n",
    "    # Find interesting pairs\n",
    "    high_cos_low_corr, low_cos_high_corr = find_interesting_chunk_pairs(df)\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Plot all points\n",
    "    plt.scatter(\n",
    "        df['cosine_sim'], \n",
    "        df['correlation'],\n",
    "        alpha=0.2,\n",
    "        s=10,\n",
    "        color='gray',\n",
    "        label='All pairs'\n",
    "    )\n",
    "    \n",
    "    # Highlight high cosine, low correlation pairs\n",
    "    plt.scatter(\n",
    "        high_cos_low_corr['cosine_sim'],\n",
    "        high_cos_low_corr['correlation'],\n",
    "        alpha=1.0,\n",
    "        s=100,\n",
    "        color='red',\n",
    "        marker='o',\n",
    "        edgecolor='black',\n",
    "        label='High cosine, low correlation'\n",
    "    )\n",
    "    \n",
    "    # Highlight low cosine, high correlation pairs\n",
    "    plt.scatter(\n",
    "        low_cos_high_corr['cosine_sim'],\n",
    "        low_cos_high_corr['correlation'],\n",
    "        alpha=1.0,\n",
    "        s=100,\n",
    "        color='blue',\n",
    "        marker='s',\n",
    "        edgecolor='black',\n",
    "        label='Low cosine, high correlation'\n",
    "    )\n",
    "    \n",
    "    # Add reference lines\n",
    "    plt.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    plt.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    plt.plot([0, 1], [0, 1], color='green', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Add regions\n",
    "    plt.axhspan(0.7, 1.0, xmax=0.3, alpha=0.1, color='blue', label='_nolegend_')\n",
    "    plt.axvspan(0.7, 1.0, ymax=0.3, alpha=0.1, color='red', label='_nolegend_')\n",
    "    \n",
    "    plt.xlabel('Cosine Similarity')\n",
    "    plt.ylabel('Pearson Correlation')\n",
    "    plt.title('Interesting Chunk Pairs: Divergent Cosine Similarity and Correlation')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return high_cos_low_corr, low_cos_high_corr\n",
    "\n",
    "# Plot interesting pairs\n",
    "high_cos_low_corr, low_cos_high_corr = plot_interesting_pairs(cosine_corr_df)\n",
    "\n",
    "# Print details about interesting pairs\n",
    "print(\"High Cosine Similarity, Low Correlation Pairs:\")\n",
    "print(high_cos_low_corr[['problem_id', 'category_i', 'category_j', 'cosine_sim', 'correlation']])\n",
    "print(\"\\nLow Cosine Similarity, High Correlation Pairs:\")\n",
    "print(low_cos_high_corr[['problem_id', 'category_i', 'category_j', 'cosine_sim', 'correlation']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_category_pairs(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Analyze which category pairs tend to have high cosine but low correlation\n",
    "    and which have low cosine but high correlation.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with cosine similarity and correlation data\n",
    "    \"\"\"\n",
    "    # Create category pair labels\n",
    "    df['category_pair'] = df.apply(\n",
    "        lambda row: f\"{row['category_i']} → {row['category_j']}\" \n",
    "        if row['category_i'] <= row['category_j'] \n",
    "        else f\"{row['category_j']} → {row['category_i']}\", \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Define interesting pairs\n",
    "    df['high_cos_low_corr'] = (df['cosine_sim'] > 0.7) & (df['correlation'] < 0.3)\n",
    "    df['low_cos_high_corr'] = (df['cosine_sim'] < 0.3) & (df['correlation'] > 0.7)\n",
    "    \n",
    "    # Count occurrences by category pair\n",
    "    high_cos_low_corr_counts = df[df['high_cos_low_corr']].groupby('category_pair').size()\n",
    "    low_cos_high_corr_counts = df[df['low_cos_high_corr']].groupby('category_pair').size()\n",
    "    \n",
    "    # Get top pairs\n",
    "    top_high_cos_low_corr = high_cos_low_corr_counts.sort_values(ascending=False).head(10)\n",
    "    top_low_cos_high_corr = low_cos_high_corr_counts.sort_values(ascending=False).head(10)\n",
    "    \n",
    "    # Plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # High cosine, low correlation\n",
    "    if not top_high_cos_low_corr.empty:\n",
    "        top_high_cos_low_corr.plot.barh(ax=ax1, color='red')\n",
    "        ax1.set_title('Top Category Pairs with High Cosine, Low Correlation')\n",
    "        ax1.set_xlabel('Count')\n",
    "        ax1.set_ylabel('Category Pair')\n",
    "    else:\n",
    "        ax1.text(0.5, 0.5, \"No pairs found\", ha='center', va='center')\n",
    "        ax1.set_title('High Cosine, Low Correlation (None Found)')\n",
    "    \n",
    "    # Low cosine, high correlation\n",
    "    if not top_low_cos_high_corr.empty:\n",
    "        top_low_cos_high_corr.plot.barh(ax=ax2, color='blue')\n",
    "        ax2.set_title('Top Category Pairs with Low Cosine, High Correlation')\n",
    "        ax2.set_xlabel('Count')\n",
    "        ax2.set_ylabel('Category Pair')\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, \"No pairs found\", ha='center', va='center')\n",
    "        ax2.set_title('Low Cosine, High Correlation (None Found)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return top_high_cos_low_corr, top_low_cos_high_corr\n",
    "\n",
    "# Analyze category pairs\n",
    "top_high_cos_low_corr, top_low_cos_high_corr = analyze_category_pairs(cosine_corr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chunk_text(problem_id, seed, chunk_idx, analysis_dir):\n",
    "    \"\"\"Load the text of a specific chunk.\"\"\"\n",
    "    chunks_file = analysis_dir / problem_id / seed / \"chunks_analyzed.json\"\n",
    "    if chunks_file.exists():\n",
    "        with open(chunks_file, 'r', encoding='utf-8') as f:\n",
    "            chunks_data = json.load(f)\n",
    "            for chunk in chunks_data:\n",
    "                if chunk['index'] == chunk_idx:\n",
    "                    return chunk['text']\n",
    "    return \"Chunk text not found\"\n",
    "\n",
    "def visualize_example_pairs(high_cos_low_corr, low_cos_high_corr, analysis_dir):\n",
    "    \"\"\"\n",
    "    Visualize example pairs of chunks with interesting properties.\n",
    "    \n",
    "    Args:\n",
    "        high_cos_low_corr: DataFrame with high cosine, low correlation pairs\n",
    "        low_cos_high_corr: DataFrame with low cosine, high correlation pairs\n",
    "        analysis_dir: Path to the analysis directory\n",
    "    \"\"\"\n",
    "    # Select one example from each category\n",
    "    high_cos_example = high_cos_low_corr.iloc[0] if not high_cos_low_corr.empty else None\n",
    "    low_cos_example = low_cos_high_corr.iloc[0] if not low_cos_high_corr.empty else None\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 1, figsize=(12, 12))\n",
    "    \n",
    "    # High cosine, low correlation example\n",
    "    if high_cos_example is not None:\n",
    "        problem_id = high_cos_example['problem_id']\n",
    "        seed = high_cos_example['seed']\n",
    "        chunk_i = high_cos_example['chunk_i']\n",
    "        chunk_j = high_cos_example['chunk_j']\n",
    "        \n",
    "        text_i = load_chunk_text(problem_id, seed, chunk_i, analysis_dir)\n",
    "        text_j = load_chunk_text(problem_id, seed, chunk_j, analysis_dir)\n",
    "        \n",
    "        axs[0].text(0.01, 0.99, f\"High Cosine ({high_cos_example['cosine_sim']:.2f}), Low Correlation ({high_cos_example['correlation']:.2f})\",\n",
    "                 fontsize=14, fontweight='bold', va='top', ha='left')\n",
    "        \n",
    "        axs[0].text(0.01, 0.90, f\"Problem: {problem_id}, Seed: {seed}\", fontsize=12, va='top', ha='left')\n",
    "        axs[0].text(0.01, 0.85, f\"Categories: {high_cos_example['category_i']} → {high_cos_example['category_j']}\", \n",
    "                 fontsize=12, va='top', ha='left')\n",
    "        \n",
    "        axs[0].text(0.01, 0.75, \"Chunk 1:\", fontsize=12, fontweight='bold', va='top', ha='left')\n",
    "        axs[0].text(0.01, 0.70, text_i[:500] + (\"...\" if len(text_i) > 500 else \"\"), \n",
    "                 fontsize=10, va='top', ha='left', wrap=True)\n",
    "        \n",
    "        axs[0].text(0.01, 0.40, \"Chunk 2:\", fontsize=12, fontweight='bold', va='top', ha='left')\n",
    "        axs[0].text(0.01, 0.35, text_j[:500] + (\"...\" if len(text_j) > 500 else \"\"), \n",
    "                 fontsize=10, va='top', ha='left', wrap=True)\n",
    "        \n",
    "        axs[0].axis('off')\n",
    "    else:\n",
    "        axs[0].text(0.5, 0.5, \"No high cosine, low correlation example found\", \n",
    "                 fontsize=14, ha='center', va='center')\n",
    "        axs[0].axis('off')\n",
    "    \n",
    "    # Low cosine, high correlation example\n",
    "    if low_cos_example is not None:\n",
    "        problem_id = low_cos_example['problem_id']\n",
    "        seed = low_cos_example['seed']\n",
    "        chunk_i = low_cos_example['chunk_i']\n",
    "        chunk_j = low_cos_example['chunk_j']\n",
    "        \n",
    "        text_i = load_chunk_text(problem_id, seed, chunk_i, analysis_dir)\n",
    "        text_j = load_chunk_text(problem_id, seed, chunk_j, analysis_dir)\n",
    "        \n",
    "        axs[1].text(0.01, 0.99, f\"Low Cosine ({low_cos_example['cosine_sim']:.2f}), High Correlation ({low_cos_example['correlation']:.2f})\",\n",
    "                 fontsize=14, fontweight='bold', va='top', ha='left')\n",
    "        \n",
    "        axs[1].text(0.01, 0.90, f\"Problem: {problem_id}, Seed: {seed}\", fontsize=12, va='top', ha='left')\n",
    "        axs[1].text(0.01, 0.85, f\"Categories: {low_cos_example['category_i']} → {low_cos_example['category_j']}\", \n",
    "                 fontsize=12, va='top', ha='left')\n",
    "        \n",
    "        axs[1].text(0.01, 0.75, \"Chunk 1:\", fontsize=12, fontweight='bold', va='top', ha='left')\n",
    "        axs[1].text(0.01, 0.70, text_i[:500] + (\"...\" if len(text_i) > 500 else \"\"), \n",
    "                 fontsize=10, va='top', ha='left', wrap=True)\n",
    "        \n",
    "        axs[1].text(0.01, 0.40, \"Chunk 2:\", fontsize=12, fontweight='bold', va='top', ha='left')\n",
    "        axs[1].text(0.01, 0.35, text_j[:500] + (\"...\" if len(text_j) > 500 else \"\"), \n",
    "                 fontsize=10, va='top', ha='left', wrap=True)\n",
    "        \n",
    "        axs[1].axis('off')\n",
    "    else:\n",
    "        axs[1].text(0.5, 0.5, \"No low cosine, high correlation example found\", \n",
    "                 fontsize=14, ha='center', va='center')\n",
    "        axs[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize example pairs\n",
    "visualize_example_pairs(high_cos_low_corr, low_cos_high_corr, analysis_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
