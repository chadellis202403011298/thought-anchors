# Thought Anchors: Which LLM Reasoning Steps Matter?

We introduce a framework for interpreting the reasoning of large language models by attributing importance to individual sentences in their chain-of-thought. Using black-box, attention-based, and causal methods, we identify key reasoning steps, which we call **thought anchors**, that disproportionately influence downstream reasoning. These anchors are typically planning or backtracking sentences. Our work offers new tools and insights for understanding multi-step reasoning in language models.

See our paper here: https://arxiv.org/abs/2506.19143

See our interactive demo here: https://www.thought-anchors.com/

See our dataset here: https://huggingface.co/datasets/uzaymacar/math-rollouts

